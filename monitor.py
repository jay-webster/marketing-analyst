import os
import asyncio
import agent
import utils
import json
import re
import ast
from types import SimpleNamespace
from datetime import datetime
from dotenv import load_dotenv
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
from google.cloud import firestore
from linkedin_tracker import LinkedInTracker

load_dotenv()
REFERENCE_DOMAIN = "navistone.com"
CACHE_COLLECTION = "discovery_cache"

try:
    project_id = os.getenv("PROJECT_ID")
    db = firestore.Client(project=project_id) if project_id else firestore.Client()
except Exception:
    db = None


# --- HELPER: POST TO SLACK ---
def post_update_to_slack(company_name, change_summary, deep_dive_url=None):
    token = os.environ.get("SLACK_BOT_TOKEN")
    channel = os.environ.get("SLACK_CHANNEL_ID")
    if not token or not channel:
        return
    client = WebClient(token=token)
    blocks = [
        {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"üö® Competitor Update: {company_name}",
                "emoji": True,
            },
        },
        {"type": "divider"},
        {"type": "section", "text": {"type": "mrkdwn", "text": f"{change_summary}"}},
    ]
    if deep_dive_url:
        blocks.append(
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"üîó <{deep_dive_url}|Visit Website> | üìÖ {datetime.now().strftime('%Y-%m-%d')}",
                    }
                ],
            }
        )
    try:
        client.chat_postMessage(
            channel=channel, blocks=blocks, text=f"Update detected for {company_name}"
        )
    except SlackApiError as e:
        print(f"‚ùå Slack Error: {e.response['error']}")


# --- HELPER: SEND EMAIL ---
def send_update_email(company_name, change_summary, deep_dive_url=None):
    if not db:
        return
    try:
        subscribers = (
            db.collection("subscribers").where("status", "==", "active").stream()
        )
        recipients = [doc.to_dict()["email"] for doc in subscribers]
    except Exception:
        return
    if not recipients:
        return

    # UPDATED: Cleaner HTML format that handles newlines better
    formatted_summary = change_summary.replace("\n", "<br>")

    html_content = f"""
    <html><body style="font-family: 'Helvetica Neue', Arial, sans-serif; color: #333;">
    <div style="max-width: 600px; margin: 0 auto; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden;">
        <div style="background-color: #0044cc; color: white; padding: 20px;">
            <h2 style="margin: 0; font-size: 20px;">üö® Competitor Alert: {company_name}</h2>
        </div>
        <div style="padding: 25px; background-color: #ffffff;">
            <div style="font-size: 15px; line-height: 1.6; color: #444;">
                {formatted_summary}
            </div>
            <hr style="border: 0; border-top: 1px solid #eee; margin: 25px 0;">
            <a href="{deep_dive_url}" style="background-color: #f1f3f4; color: #0044cc; padding: 10px 15px; text-decoration: none; border-radius: 4px; font-size: 13px; font-weight: bold;">Visit Competitor Website &rarr;</a>
        </div>
        <div style="background-color: #f8f9fa; padding: 15px; text-align: center; color: #888; font-size: 11px;">
            Generated by Marketing Analyst Agent ‚Ä¢ {datetime.now().strftime('%Y-%m-%d')}
        </div>
    </div>
    </body></html>
    """
    for email in recipients:
        utils.send_email(
            subject=f"‚ö†Ô∏è Strategy Update: {company_name}",
            recipient_email=email,
            body_text=change_summary,
            html_body=html_content,
        )


# --- HELPER: BASELINE REPORT ---
def send_baseline_report(new_subscriber_email):
    print(f"üì® Generating Baseline Report for {new_subscriber_email}...")
    if not db:
        return
    competitor_cards = ""
    count = 0
    try:
        track_docs = db.collection("competitors").stream()
        for doc in track_docs:
            data = doc.to_dict()
            domain = doc.id
            if REFERENCE_DOMAIN.lower() in domain.lower():
                continue

            content = data.get("content", {})
            val_prop = content.get("value_proposition", "Pending Analysis...")
            news_data = content.get("latest_news", {})

            def format_section(title, items):
                if not items or items == "N/A":
                    return ""
                if isinstance(items, list) and items:
                    list_html = "".join(
                        [f"<li style='margin-bottom:6px;'>{i}</li>" for i in items]
                    )
                    return f"<div style='margin-top:12px;'><strong style='font-size:12px; color:#0044cc; text-transform:uppercase;'>{title}</strong><ul style='margin:5px 0 0 20px; font-size:13px; color:#444;'>{list_html}</ul></div>"
                elif isinstance(items, str) and len(items) > 5:
                    return f"<div style='margin-top:12px;'><strong style='font-size:12px; color:#0044cc; text-transform:uppercase;'>{title}</strong><p style='margin:2px 0 0 0; font-size:13px; color:#444;'>{items}</p></div>"
                return ""

            news_html = ""
            news_html += format_section("üöÄ Launches", news_data.get("launches"))
            news_html += format_section(
                "ü§ù Partnerships", news_data.get("partnerships")
            )
            news_html += format_section("üëî Leadership", news_data.get("leadership"))
            news_html += format_section("üí∞ Funding", news_data.get("funding"))

            if not news_html:
                news_html = "<p style='font-size:13px; color:#888; font-style:italic;'>No major recent announcements detected.</p>"

            competitor_cards += f"""
            <div style="border: 1px solid #e0e0e0; border-radius: 8px; padding: 25px; margin-bottom: 30px; background-color: #ffffff; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
                <div style="display:flex; justify-content:space-between; align-items:center; border-bottom: 1px solid #f0f0f0; padding-bottom: 15px; margin-bottom: 15px;">
                    <h3 style="margin: 0; color: #2c3e50; font-size: 20px;">{data.get("name", domain.split('.')[0].capitalize())}</h3>
                    <a href="https://{domain}" style="font-size: 12px; color: #0044cc; text-decoration: none; font-weight: bold;">Visit Site &rarr;</a>
                </div>
                
                <div style="margin-bottom: 20px;">
                    <strong style="color: #555; font-size: 11px; text-transform: uppercase; letter-spacing: 0.5px;">Core Strategy</strong>
                    <p style="margin-top: 5px; font-size: 14px; line-height: 1.6; color: #333;">{val_prop}</p>
                </div>

                <div style="background-color: #f9fafb; padding: 20px; border-radius: 6px;">
                    {news_html}
                </div>
            </div>
            """
            count += 1

    except Exception as e:
        print(f"‚ùå Error gathering baseline: {e}")
        return

    if count == 0:
        return

    html_content = f"""
    <html>
        <body style="font-family: 'Helvetica Neue', Arial, sans-serif; color: #333; background-color: #f4f4f4; padding: 40px 20px;">
            <div style="max-width: 700px; margin: 0 auto;">
                <div style="text-align: center; margin-bottom: 40px;">
                    <h1 style="margin: 0; font-size: 28px; color: #2c3e50; letter-spacing: -0.5px;">Competitive Intelligence Dossier</h1>
                    <p style="margin: 10px 0 0 0; color: #7f8c8d; font-size: 14px;">Active Surveillance Report ‚Ä¢ {datetime.now().strftime('%B %d, %Y')}</p>
                </div>
                {competitor_cards}
                <div style="text-align: center; color: #aaa; font-size: 12px; margin-top: 40px;">
                    Generated by Navistone Marketing Analyst Agent
                </div>
            </div>
        </body>
    </html>
    """

    utils.send_email(
        subject="üìà Baseline Report: Active Surveillance Targets",
        recipient_email=new_subscriber_email,
        body_text="Your baseline report is attached (HTML).",
        html_body=html_content,
    )
    print(f"‚úÖ Baseline report sent to {new_subscriber_email}")


# --- CORE LOGIC: DISCOVER (WITH SEED-BASED CONTEXT) ---
async def discover_competitors(target_domain):
    print(f"üî≠ Starting Deep Discovery for {target_domain}...")

    # 1. FETCH KNOWN COMPETITORS (The "Seed" Data)
    active_tracked_domains = []
    known_competitors_names = []

    if db:
        docs = db.collection("competitors").stream()
        for doc in docs:
            # Clean domain for filtering
            d_clean = doc.id.lower().replace("www.", "").split(".")[0]
            active_tracked_domains.append(d_clean)

            # Get name for Context Seeding (Exclude the target itself)
            if target_domain.lower() not in doc.id.lower():
                data = doc.to_dict()
                name = data.get("name", d_clean.capitalize())
                known_competitors_names.append(name)

    target_clean = target_domain.lower().replace("www.", "").split(".")[0]
    active_tracked_domains.append(target_clean)

    # 2. FETCH TARGET INTELLIGENCE
    try:
        print(f"   -> Profiling {target_domain} via Search & Scrape...")
        search_dump = agent.google_search(
            f"What does {target_domain} do? business model competitors"
        )
        scrape_dump = agent.scrape_website(f"https://{target_domain}")
    except Exception as e:
        print(f"   -> Profiling warning: {e}")
        search_dump = "Search failed."
        scrape_dump = "Scrape failed."

    # 3. BUILD SEED CONTEXT
    seed_context = ""
    if known_competitors_names:
        seeds = ", ".join(
            known_competitors_names[:5]
        )  # Limit to top 5 to avoid prompt overflow
        seed_context = (
            f"\n--- SEED DATA (CRITICAL) ---\n"
            f"The user is ALREADY tracking these competitors: {seeds}.\n"
            f"This indicates the user is interested in a SPECIFIC niche (e.g. Programmatic Direct Mail) rather than generic marketing.\n"
            f"Use these seeds to calibrate your search. If {target_domain} does what {seeds} do, focus on that overlap.\n"
        )

    # 4. CONSTRUCT INTELLIGENT PROMPT
    prompt = (
        f"I need to identify 10 direct competitors for {target_domain}.\n"
        f"First, use the provided intelligence below to build a precise Industry Profile for {target_domain}.\n"
        f"Then, find companies that solve the SAME problem for the SAME customer base.\n"
        f"{seed_context}\n"
        f"--- TARGET INTELLIGENCE ---\n"
        f"{search_dump}\n\n"
        f"--- WEBSITE CONTENT ---\n"
        f"{scrape_dump[:10000]}\n\n"
        f"--- INSTRUCTIONS ---\n"
        f"Return valid JSON with keys: 'industry_profile' (string) and 'competitors' (list of {{name, domain, reason}})."
    )

    try:
        raw_response = await agent.run_agent_turn(prompt, [], headless=True)
        data = {}
        json_match = re.search(r"\{.*\}", raw_response, re.DOTALL)
        if json_match:
            try:
                data = json.loads(json_match.group(0))
            except:
                try:
                    data = ast.literal_eval(json_match.group(0))
                except:
                    pass

        candidates = data.get("competitors", [])

        # --- FALLBACK MECHANISM ---
        if not candidates:
            print(
                "‚ö†Ô∏è Primary discovery returned 0 candidates. Triggering Brainstorm Fallback..."
            )
            fallback_prompt = (
                f"I need 5 competitors for {target_domain}. "
                f"Based on the industry: {data.get('industry_profile', 'Unknown')}, brainstorm 5 companies.\n"
                f"Return JSON with keys: 'industry_profile', 'competitors' (list of {{name, domain, reason}})."
            )
            raw_response = await agent.run_agent_turn(
                fallback_prompt, [], headless=True
            )
            json_match = re.search(r"\{.*\}", raw_response, re.DOTALL)
            if json_match:
                try:
                    data = json.loads(json_match.group(0))
                except:
                    pass
            candidates = data.get("competitors", [])

        valid_competitors = []
        for comp in candidates:
            if not comp.get("domain"):
                clean_name = comp.get("name", "").replace(" ", "").lower()
                comp["domain"] = f"{clean_name}.com"

            raw_domain = comp["domain"].lower()
            candidate_stem = raw_domain.replace("www.", "").split(".")[0]

            # STRICT FILTERING
            is_banned = False
            for banned_key in active_tracked_domains:
                if banned_key == candidate_stem:
                    is_banned = True
                    break

            if not is_banned:
                valid_competitors.append(comp)

        final_list = valid_competitors[:5]

        if db and final_list:
            db.collection(CACHE_COLLECTION).document(target_domain).set(
                {
                    "industry_profile": data.get("industry_profile", "Unknown"),
                    "competitors": final_list,
                    "dismissed": [],
                    "last_updated": datetime.now(),
                }
            )
            print(f"‚úÖ Found {len(final_list)} unique competitors.")
            return final_list

        return []

    except Exception as e:
        print(f"‚ùå Discovery failed: {e}")
        return []


# --- CORE LOGIC: REFRESH ---
async def refresh_competitors(target_domain, target_count=5, retry_level=0):
    print(f"üîÑ Refreshing list for {target_domain} (Attempt: {retry_level})...")
    if not db:
        return []

    doc_ref = db.collection(CACHE_COLLECTION).document(target_domain)
    doc = doc_ref.get()

    existing_competitors = []
    dismissed_domains = []
    industry_profile = "a specific industry"

    if doc.exists:
        data = doc.to_dict()
        existing_competitors = data.get("competitors", [])
        dismissed_domains = data.get("dismissed", [])
        industry_profile = data.get("industry_profile", industry_profile)

    active_tracked_domains = []
    active_docs = db.collection("competitors").stream()
    active_tracked_domains = [
        d.id.lower().replace("www.", "").split(".")[0] for d in active_docs
    ]

    needed = target_count - len(existing_competitors)
    if needed <= 0:
        return existing_competitors

    ask_for = max(needed, 5) + (retry_level * 3)

    # UPDATED: Add seed context AND fix the "Reason" bug
    seeds = ", ".join(
        [
            d
            for d in active_tracked_domains
            if d not in [c["name"].lower() for c in existing_competitors]
        ][:5]
    )

    prompt = (
        f"Find {ask_for} NEW competitors for {target_domain} (Industry: {industry_profile}).\n"
        f"CONTEXT: The user is specifically tracking: {seeds}. \n"
        f"IGNORE generic competitors. Find companies that match the specific business model of the seeds (e.g. Programmatic Direct Mail, Retargeting).\n"
        f"They must be different from: {', '.join([c['name'] for c in existing_competitors])}.\n"
        f"INSTRUCTION: Return JSON with key 'competitors' (list of {{name, domain, reason}}). "
        f"The 'reason' must explicitly state how they resemble the seed companies."
    )

    try:
        raw_response = await agent.run_agent_turn(prompt, [], headless=True)

        data = {}
        json_match = re.search(r"\{.*\}", raw_response, re.DOTALL)
        if json_match:
            try:
                data = json.loads(json_match.group(0))
            except:
                try:
                    data = ast.literal_eval(json_match.group(0))
                except:
                    data = {}

        candidates = data.get("competitors", [])
        valid_new = []

        for cand in candidates:
            if not cand.get("domain"):
                cand["domain"] = f"{cand.get('name','').replace(' ','').lower()}.com"

            raw_domain = cand.get("domain", "").lower()
            candidate_stem = raw_domain.replace("www.", "").split(".")[0]

            is_banned = False
            for banned_key in active_tracked_domains + dismissed_domains:
                banned_stem = (
                    banned_key.replace("www.", "").split(".")[0]
                    if "." in banned_key
                    else banned_key
                )
                if banned_stem == candidate_stem:
                    is_banned = True
                    break

            for curr in existing_competitors:
                curr_stem = (
                    curr.get("domain", "").lower().replace("www.", "").split(".")[0]
                )
                if curr_stem == candidate_stem:
                    is_banned = True

            if not is_banned:
                valid_new.append(cand)

        final_additions = valid_new[:needed]

        if final_additions:
            full_list = existing_competitors + final_additions
            doc_ref.update({"competitors": full_list, "last_updated": datetime.now()})
            return full_list
        else:
            if retry_level < 2:
                return await refresh_competitors(
                    target_domain, target_count, retry_level + 1
                )
            return existing_competitors

    except Exception:
        return existing_competitors


# --- CORE LOGIC: REMOVE ---
def remove_competitor_from_cache(target_domain, competitor_domain_to_remove):
    if not db:
        return

    try:
        doc_ref = db.collection(CACHE_COLLECTION).document(target_domain)
        doc = doc_ref.get()

        if doc.exists:
            data = doc.to_dict()
            current_list = data.get("competitors", [])
            dismissed_list = data.get("dismissed", [])

            new_list = [
                c
                for c in current_list
                if c.get("domain") != competitor_domain_to_remove
            ]

            if competitor_domain_to_remove not in dismissed_list:
                dismissed_list.append(competitor_domain_to_remove)

            doc_ref.update({"competitors": new_list, "dismissed": dismissed_list})
            print(f"üóëÔ∏è Blacklisted {competitor_domain_to_remove} for {target_domain}")
    except Exception as e:
        print(f"‚ùå Error updating cache: {e}")


# --- ANALYSIS HELPERS ---
async def check_linkedin_updates(company_name, domain):
    tracker = LinkedInTracker(agent_module=agent, use_mock=False)
    try:
        return await tracker.get_company_updates(company_name, domain)
    except:
        return None


async def analyze_competitor_website(domain):
    print(f"üîé [Direct-Fetch] Starting Analysis for {domain}...")

    # 1. DIRECT PYTHON FETCH: Google Search (Bypasses Agent Laziness)
    try:
        print(f"   -> Fetching News...")
        search_query = f"{domain} latest strategic news press releases"
        search_data = agent.google_search(search_query)
    except Exception as e:
        search_data = f"Search Failed: {e}"

    # 2. DIRECT PYTHON FETCH: Website Scrape
    try:
        print(f"   -> Fetching Website...")
        url = f"https://{domain}"
        scrape_data = agent.scrape_website(url)
    except Exception as e:
        scrape_data = f"Scrape Failed: {e}"

    # 3. FEED RAW DATA TO AGENT (Robust Prompt)
    prompt = f"""
    ACT AS A CHIEF MARKETING OFFICER.
    I have collected the following raw intelligence for {domain}.
    Your job is to extract the strategic profile and recent news.

    --- DATA SOURCE 1: GOOGLE SEARCH RESULTS ---
    {search_data}
    
    --- DATA SOURCE 2: WEBSITE CONTENT ---
    {scrape_data[:12000]} 
    
    --- INSTRUCTIONS ---
    1. IGNORE any 'Scrape Failed' or 'Access Denied' errors if other data exists. Use whatever you have.
    2. LOOK FOR URLs: If a news item comes from Google Search, capture the source URL.
    3. Return valid JSON with these keys:
    - 'name': Company Name (string)
    - 'value_proposition': 2 sentences on their core strategy (string).
    - 'latest_news': Object with keys:
        - 'launches': List of strings formatted as: "Headline - Details (Source: <URL>)"
        - 'partnerships': List of strings formatted as: "Partner Name - Details (Source: <URL>)"
        - 'leadership': List of strings formatted as: "New Role - Name (Source: <URL>)"
        - 'funding': List of strings formatted as: "Amount/Type - Details (Source: <URL>)"
    """

    try:
        raw = await agent.run_agent_turn(prompt, [], headless=True)
        match = re.search(r"\{.*\}", raw, re.DOTALL)
        if match:
            data = json.loads(match.group(0))
            if data.get("value_proposition"):
                return SimpleNamespace(**data)
    except Exception as e:
        print(f"‚ùå Analysis Parse Error (First Attempt): {e}")

    # --- FALLBACK: MEMORY RECOVERY ---
    print(f"‚ö†Ô∏è Direct Analysis Failed. Attempting Memory Fallback for {domain}...")
    try:
        fallback_prompt = f"""
        I cannot access the website for {domain} right now.
        Based on your INTERNAL KNOWLEDGE and training data:
        1. What is the core value proposition of {domain}?
        2. What are the most common recent news themes for this company (general)?
        
        Return valid JSON with keys:
        - 'name': '{domain}'
        - 'value_proposition': (Your best summary)
        - 'latest_news': {{'launches': ['(Based on general knowledge)'], 'partnerships': [], 'leadership': [], 'funding': []}}
        """
        raw = await agent.run_agent_turn(fallback_prompt, [], headless=True)
        match = re.search(r"\{.*\}", raw, re.DOTALL)
        if match:
            return SimpleNamespace(**json.loads(match.group(0)))
    except Exception:
        pass

    return SimpleNamespace(
        name=domain, value_proposition="Analysis currently unavailable.", latest_news={}
    )


# --- MAIN JOB ---
async def run_daily_brief():
    print(f"üöÄ Starting Daily Surveillance: {datetime.now()}")
    if not db:
        return

    comp_docs = db.collection("competitors").stream()
    competitors = [doc.id for doc in comp_docs]
    memory = utils.load_memory()

    if REFERENCE_DOMAIN in competitors:
        competitors.remove(REFERENCE_DOMAIN)

    updates_detected = 0

    for domain in competitors:
        print(f"--- Surveillance: {domain} ---")
        company_name = domain.split(".")[0].capitalize()

        # 1. Run Analysis
        website_result = await analyze_competitor_website(domain)
        linkedin_result = await check_linkedin_updates(company_name, domain)

        prev_data = memory.get(domain, {})
        if not prev_data:
            doc_snap = db.collection("competitors").document(domain).get()
            if doc_snap.exists:
                prev_data = doc_snap.to_dict()

        if isinstance(prev_data, str):
            prev_val_prop = prev_data
        else:
            prev_val_prop = prev_data.get("content", {}).get("value_proposition")
        current_val_prop = getattr(website_result, "value_proposition", "N/A")

        website_changed = False
        if "Pending" in str(prev_val_prop) and "unavailable" not in current_val_prop:
            website_changed = True
        elif prev_val_prop and prev_val_prop != "N/A":
            if (current_val_prop != prev_val_prop) and (
                "unavailable" not in current_val_prop
            ):
                website_changed = True

        li_summary = (
            linkedin_result.summary_text
            if (linkedin_result and len(linkedin_result.summary_text) > 50)
            else ""
        )
        news_found = bool(li_summary and "No recent updates" not in li_summary)

        # 3. SAVE RICH DATA
        full_company_data = {
            "name": getattr(website_result, "name", domain),
            "content": {
                "value_proposition": current_val_prop,
                "latest_news": getattr(website_result, "latest_news", {}),
                "solutions": getattr(website_result, "solutions", "N/A"),
                "industries": getattr(website_result, "industries", "N/A"),
            },
            "linkedin_update": li_summary or "No recent updates.",
            "last_updated": datetime.now().isoformat(),
        }

        if "unavailable" not in current_val_prop:
            db.collection("competitors").document(domain).set(
                full_company_data, merge=True
            )
            print(f"üíæ Saved deep data for {domain}")

        if website_changed or news_found:
            updates_detected += 1

            # 1. FORMAT THE NEWS DATA (Rich with Links)
            news_obj = getattr(website_result, "latest_news", {})
            news_text = ""
            if isinstance(news_obj, dict):
                for category, items in news_obj.items():
                    if items and items != "None" and items != ["None"]:
                        if isinstance(items, list):
                            items = "\n  ".join(items)
                        news_text += f"- {category.capitalize()}: {items}\n"

            # 2. BUILD THE CONTEXT
            change_context = (
                f"COMPETITOR: {company_name}\n"
                f"LINKEDIN SUMMARY: {li_summary}\n"
                f"WEBSITE STRATEGY: {current_val_prop}\n"
                f"RECENT NEWS/PRESS:\n{news_text}"
            )

            # 3. THE PROMPT
            prompt_with_context = (
                f"You are a Strategy Consultant creating an Executive Briefing for {company_name}.\n"
                f"Use the following data (ignore generic marketing fluff):\n\n"
                f"{change_context}\n\n"
                f"--- INSTRUCTIONS ---\n"
                f"Write exactly 2 bullet points.\n"
                f"For each bullet, use this format:\n"
                f"**[Event/Update Name]**: [What happened]. **Strategic Implication**: [Why it matters to a competitor]. [Source: URL if available]\n"
                f"DO NOT use conversational filler like 'Okay'. Start directly with the text."
            )

            analyst_note = await agent.run_agent_turn(
                prompt_with_context, [], headless=False
            )

            post_update_to_slack(
                company_name, analyst_note, deep_dive_url=f"https://{domain}"
            )
            send_update_email(
                company_name, analyst_note, deep_dive_url=f"https://{domain}"
            )

        await asyncio.sleep(2)

    utils.save_memory(memory)
    print(f"üíæ Surveillance Complete. {updates_detected} updates sent.")


if __name__ == "__main__":
    asyncio.run(run_daily_brief())
