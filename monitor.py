import os
import asyncio
import agent
import utils
import json
import re
import ast
from types import SimpleNamespace
from datetime import datetime
from dotenv import load_dotenv
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
from google.cloud import firestore
from linkedin_tracker import LinkedInTracker

load_dotenv()
REFERENCE_DOMAIN = "navistone.com"
CACHE_COLLECTION = "discovery_cache"

try:
    project_id = os.getenv("PROJECT_ID")
    db = firestore.Client(project=project_id) if project_id else firestore.Client()
except Exception:
    db = None


# --- HELPER: POST TO SLACK ---
def post_update_to_slack(company_name, change_summary, deep_dive_url=None):
    token = os.environ.get("SLACK_BOT_TOKEN")
    channel = os.environ.get("SLACK_CHANNEL_ID")
    if not token or not channel:
        return
    client = WebClient(token=token)
    blocks = [
        {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"üö® Competitor Update: {company_name}",
                "emoji": True,
            },
        },
        {"type": "divider"},
        {"type": "section", "text": {"type": "mrkdwn", "text": f"{change_summary}"}},
    ]
    if deep_dive_url:
        blocks.append(
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"üîó <{deep_dive_url}|Visit Website> | üìÖ {datetime.now().strftime('%Y-%m-%d')}",
                    }
                ],
            }
        )
    try:
        client.chat_postMessage(
            channel=channel, blocks=blocks, text=f"Update detected for {company_name}"
        )
    except SlackApiError as e:
        print(f"‚ùå Slack Error: {e.response['error']}")


# --- HELPER: SEND EMAIL ---
def send_update_email(company_name, change_summary, deep_dive_url=None):
    if not db:
        return
    try:
        subscribers = (
            db.collection("subscribers").where("status", "==", "active").stream()
        )
        recipients = [doc.to_dict()["email"] for doc in subscribers]
    except Exception:
        return
    if not recipients:
        return

    html_content = f"""
    <html><body style="font-family: Arial, sans-serif; color: #333;"><div style="max-width: 600px; margin: 0 auto; border: 1px solid #ddd; border-radius: 8px; overflow: hidden;"><div style="background-color: #0044cc; color: white; padding: 20px;"><h2 style="margin: 0;">üö® Competitor Alert: {company_name}</h2></div><div style="padding: 20px;"><p style="font-size: 16px; line-height: 1.5;"><strong>Analyst Note:</strong><br>{change_summary.replace(chr(10), '<br>')}</p><hr style="border: 0; border-top: 1px solid #eee; margin: 20px 0;"><p style="font-size: 12px; color: #777;">Report generated by Marketing Analyst Agent</p></div></div></body></html>
    """
    for email in recipients:
        utils.send_email(
            subject=f"‚ö†Ô∏è Update: {company_name} Strategy Shift",
            recipient_email=email,
            body_text=change_summary,
            html_body=html_content,
        )


# --- HELPER: BASELINE REPORT ---
def send_baseline_report(new_subscriber_email):
    print(f"üì® Generating Baseline Report for {new_subscriber_email}...")
    if not db:
        return
    competitor_data = []
    try:
        track_docs = db.collection("competitors").stream()
        for doc in track_docs:
            data = doc.to_dict()
            domain = doc.id
            if REFERENCE_DOMAIN.lower() in domain.lower():
                continue
            content = data.get("content", {})
            val_prop = content.get("value_proposition", "Pending Analysis...")
            if len(val_prop) > 200:
                val_prop = val_prop[:200] + "..."
            competitor_data.append(
                {
                    "name": data.get("name", domain.split(".")[0].capitalize()),
                    "domain": domain,
                    "val_prop": val_prop,
                }
            )
    except Exception as e:
        print(f"‚ùå Error gathering baseline: {e}")
        return

    if not competitor_data:
        return
    list_items = ""
    for comp in competitor_data:
        list_items += f'<li style="margin-bottom: 20px; border-bottom: 1px solid #eee; padding-bottom: 10px;"><strong style="font-size: 16px;">{comp["name"]}</strong> <a href="https://{comp["domain"]}" style="color: #0044cc; text-decoration: none; font-size: 12px;">(Visit)</a><br><span style="color: #555; font-size: 14px; font-style: italic;">{comp["val_prop"]}</span></li>'

    html_content = f"""
    <html><body style="font-family: Arial, sans-serif; color: #333;"><div style="max-width: 600px; margin: 0 auto; border: 1px solid #ddd; border-radius: 8px; overflow: hidden;"><div style="background-color: #2c3e50; color: white; padding: 25px;"><h1 style="margin: 0; font-size: 22px;">Welcome to Market Intel</h1><p style="margin: 5px 0 0 0; opacity: 0.8;">Your surveillance feed is active.</p></div><div style="padding: 25px;"><p>We are tracking <strong>{len(competitor_data)} competitors</strong> for you:</p><ul style="padding-left: 20px;">{list_items}</ul></div></div></body></html>
    """
    utils.send_email(
        subject="üìà Baseline Report: Active Surveillance Targets",
        recipient_email=new_subscriber_email,
        body_text="Your baseline report is attached (HTML).",
        html_body=html_content,
    )
    print(f"‚úÖ Baseline report sent to {new_subscriber_email}")


# --- CORE LOGIC: DISCOVER (Enhanced Filter) ---
async def discover_competitors(target_domain):
    print(f"üî≠ Starting Deep Discovery for {target_domain}...")

    # 1. Fetch ALL actively tracked domains for fuzzy matching
    active_tracked_domains = []
    if db:
        docs = db.collection("competitors").stream()
        # Clean: "https://www.pebblepost.com" -> "pebblepost"
        active_tracked_domains = [
            d.id.lower().replace("www.", "").split(".")[0] for d in docs
        ]

    # Also add the target itself
    target_clean = target_domain.lower().replace("www.", "").split(".")[0]
    active_tracked_domains.append(target_clean)

    banned_str = ", ".join(active_tracked_domains)

    prompt = (
        f"Identify top 5 direct competitors for {target_domain}. "
        f"DO NOT RETURN COMPANIES MATCHING THESE KEYWORDS: {banned_str}.\n"
        f"STEP 1: Analyze {target_domain} industry.\n"
        f"STEP 2: Find 5 NEW competitors.\n"
        f"STEP 3: Return JSON keys: 'industry_profile', 'competitors' (list of {{name, domain, reason}})."
    )

    try:
        raw_response = await agent.run_agent_turn(prompt, [], headless=True)
        # Robust Parsing
        data = {}
        json_match = re.search(r"\{.*\}", raw_response, re.DOTALL)
        if json_match:
            try:
                data = json.loads(json_match.group(0))
            except:
                try:
                    data = ast.literal_eval(json_match.group(0))
                except:
                    return []

        valid_competitors = []
        candidates = data.get("competitors", [])

        for comp in candidates:
            # Domain Fix
            if not comp.get("domain"):
                clean_name = comp.get("name", "").replace(" ", "").lower()
                comp["domain"] = f"{clean_name}.com"

            raw_domain = comp["domain"].lower()
            clean_check = raw_domain.replace("www.", "").split(".")[0]

            # FUZZY BAN CHECK
            # If "pebblepost" is in active_tracked, block "pebblepost.com" AND "www.pebblepost.com"
            is_banned = False
            for banned_key in active_tracked_domains:
                if (
                    banned_key in raw_domain
                    or banned_key in comp.get("name", "").lower()
                ):
                    is_banned = True
                    break

            if is_banned:
                print(f"üôà Skipping tracked competitor: {comp.get('name')}")
                continue

            valid_competitors.append(comp)

        if db and valid_competitors:
            db.collection(CACHE_COLLECTION).document(target_domain).set(
                {
                    "industry_profile": data.get("industry_profile", "Unknown"),
                    "competitors": valid_competitors,
                    "dismissed": [],
                    "last_updated": datetime.now(),
                }
            )
            return valid_competitors
        return []
    except Exception as e:
        print(f"‚ùå Discovery failed: {e}")
        return []


# --- CORE LOGIC: REFRESH (Same Fuzzy Logic) ---
async def refresh_competitors(target_domain, target_count=5, retry_level=0):
    # (Simplified for brevity - assumes logic mirrors discovery above)
    # The key fix is ensuring 'banned_list' uses the fuzzy check logic
    # ...
    # [Rest of refresh logic is fine, assuming discovery logic handles the heavy lifting of cache population]
    # For now, let's keep the existing refresh logic but ensure it saves correctly.
    # The main issue reported was Discovery suggesting duplicates, which the function above fixes.
    return (
        []
    )  # Stub for brevity in this answer, ensure you keep your working refresh logic!


# --- ANALYSIS HELPERS ---
async def check_linkedin_updates(company_name, domain):
    tracker = LinkedInTracker(agent_module=agent, use_mock=False)
    try:
        return await tracker.get_company_updates(company_name, domain)
    except:
        return None


async def analyze_competitor_website(domain):
    # 1. Try Scrape
    urls = [f"https://{domain}", f"https://www.{domain}"]
    for url in urls:
        try:
            raw = await agent.run_agent_turn(
                f"Analyze {url}. Return JSON: name, value_proposition (1 sentence), solutions.",
                [],
                headless=True,
            )
            match = re.search(r"\{.*\}", raw, re.DOTALL)
            if match:
                data = json.loads(match.group(0))
                if (
                    data.get("value_proposition")
                    and "unavailable" not in data["value_proposition"]
                ):
                    return SimpleNamespace(**data)
        except:
            pass

    # 2. Strong Fallback (Internal Knowledge)
    print(f"‚ö†Ô∏è Scrape failed for {domain}. Using internal knowledge fallback.")
    try:
        raw = await agent.run_agent_turn(
            f"You are a CMO. Write a 1-sentence value proposition for {domain}. Return JSON: {{'name': '{domain}', 'value_proposition': '...'}}",
            [],
            headless=True,
        )
        match = re.search(r"\{.*\}", raw, re.DOTALL)
        if match:
            return SimpleNamespace(**json.loads(match.group(0)))
    except:
        pass

    return SimpleNamespace(
        name=domain, value_proposition="Analysis currently unavailable."
    )


# --- MAIN JOB: RUN DAILY BRIEF (Force Save) ---
async def run_daily_brief():
    print(f"üöÄ Starting Daily Surveillance: {datetime.now()}")
    if not db:
        return

    comp_docs = db.collection("competitors").stream()
    competitors = [doc.id for doc in comp_docs]
    memory = utils.load_memory()

    if REFERENCE_DOMAIN in competitors:
        competitors.remove(REFERENCE_DOMAIN)

    updates_detected = 0

    for domain in competitors:
        print(f"--- Surveillance: {domain} ---")
        company_name = domain.split(".")[0].capitalize()

        # 1. Run Analysis
        website_result = await analyze_competitor_website(domain)
        linkedin_result = await check_linkedin_updates(company_name, domain)

        prev_data = memory.get(domain, {})
        # Check DB if memory empty
        if not prev_data:
            doc_snap = db.collection("competitors").document(domain).get()
            if doc_snap.exists:
                prev_data = doc_snap.to_dict()

        if isinstance(prev_data, str):
            prev_val_prop = prev_data
        else:
            prev_val_prop = prev_data.get("content", {}).get("value_proposition")

        current_val_prop = getattr(website_result, "value_proposition", "N/A")

        # FORCE UPDATE: If previous was "Pending", TREAT AS CHANGE to overwrite it
        website_changed = False
        if "Pending" in str(prev_val_prop) and "unavailable" not in current_val_prop:
            website_changed = True  # First real analysis complete
        elif prev_val_prop and prev_val_prop != "N/A":
            if (current_val_prop != prev_val_prop) and (
                "unavailable" not in current_val_prop
            ):
                website_changed = True

        li_summary = (
            linkedin_result.summary_text
            if (linkedin_result and len(linkedin_result.summary_text) > 50)
            else ""
        )
        news_found = bool(li_summary and "No recent updates" not in li_summary)

        # 3. ALWAYS SAVE TO DB (Persistence Fix)
        full_company_data = {
            "name": getattr(website_result, "name", domain),
            "content": {
                "value_proposition": current_val_prop,
                "solutions": getattr(website_result, "solutions", "N/A"),
                "industries": getattr(website_result, "industries", "N/A"),
            },
            "linkedin_update": li_summary or "No recent updates.",
            "last_updated": datetime.now().isoformat(),
        }

        # Save to Firestore unconditionally to fix "Pending" status
        if "unavailable" not in current_val_prop:
            db.collection("competitors").document(domain).set(
                full_company_data, merge=True
            )
            print(f"üíæ Saved data for {domain}")

        # 4. REPORT
        if website_changed or news_found:
            print(f"üîî CHANGE DETECTED for {company_name}!")
            updates_detected += 1
            # ... (Send notifications logic same as before) ...

        await asyncio.sleep(2)  # Faster interval

    print(f"üíæ Surveillance Complete. {updates_detected} updates sent.")


if __name__ == "__main__":
    asyncio.run(run_daily_brief())
