import os
import asyncio
import agent
import utils
import json
import re
import ast
from types import SimpleNamespace
from datetime import datetime
from dotenv import load_dotenv
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
from google.cloud import firestore
from linkedin_tracker import LinkedInTracker

load_dotenv()
REFERENCE_DOMAIN = "navistone.com"
CACHE_COLLECTION = "discovery_cache"

# --- HELPER: DATABASE ---
try:
    project_id = os.getenv("PROJECT_ID")
    db = firestore.Client(project=project_id) if project_id else firestore.Client()
except Exception:
    db = None


# --- HELPER: POST TO SLACK (Rich Block Kit) ---
def post_update_to_slack(company_name, change_summary, deep_dive_url=None):
    token = os.environ.get("SLACK_BOT_TOKEN")
    channel = os.environ.get("SLACK_CHANNEL_ID")
    if not token or not channel:
        return

    client = WebClient(token=token)

    blocks = [
        {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"üö® Competitor Update: {company_name}",
                "emoji": True,
            },
        },
        {"type": "divider"},
        {"type": "section", "text": {"type": "mrkdwn", "text": f"{change_summary}"}},
    ]

    if deep_dive_url:
        blocks.append(
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": f"üîó <{deep_dive_url}|Visit Website> | üìÖ {datetime.now().strftime('%Y-%m-%d')}",
                    }
                ],
            }
        )

    try:
        client.chat_postMessage(
            channel=channel, blocks=blocks, text=f"Update detected for {company_name}"
        )
        print(f"‚úÖ Posted update for {company_name} to Slack.")
    except SlackApiError as e:
        print(f"‚ùå Slack Error: {e.response['error']}")


# --- HELPER: SEND EMAIL (Rich HTML) ---
def send_update_email(company_name, change_summary, deep_dive_url=None):
    if not db:
        return
    try:
        subscribers = (
            db.collection("subscribers").where("status", "==", "active").stream()
        )
        recipients = [doc.to_dict()["email"] for doc in subscribers]
    except Exception:
        return

    if not recipients:
        return

    html_content = f"""
    <html>
        <body style="font-family: Arial, sans-serif; color: #333;">
            <div style="max-width: 600px; margin: 0 auto; border: 1px solid #ddd; border-radius: 8px; overflow: hidden;">
                <div style="background-color: #0044cc; color: white; padding: 20px;">
                    <h2 style="margin: 0;">üö® Competitor Alert: {company_name}</h2>
                </div>
                <div style="padding: 20px;">
                    <p style="font-size: 16px; line-height: 1.5;">
                        <strong>Analyst Note:</strong><br>
                        {change_summary.replace(chr(10), '<br>')}
                    </p>
                    <hr style="border: 0; border-top: 1px solid #eee; margin: 20px 0;">
                    <p style="font-size: 12px; color: #777;">
                        Report generated by Marketing Analyst Agent ‚Ä¢ {datetime.now().strftime('%Y-%m-%d')}
                    </p>
                </div>
            </div>
        </body>
    </html>
    """

    for email in recipients:
        utils.send_email(
            subject=f"‚ö†Ô∏è Update: {company_name} Strategy Shift",
            recipient=email,
            body=change_summary,
            attachment_bytes=None,
            filename=None,
            html_body=html_content,
        )
    print(f"üìß Sent email update for {company_name} to {len(recipients)} subscribers.")


# --- CORE LOGIC: DISCOVER (Bulletproof Parsing) ---
async def discover_competitors(target_domain):
    print(f"üî≠ Starting Deep Discovery for {target_domain}...")

    if db:
        doc_ref = db.collection(CACHE_COLLECTION).document(target_domain)
        doc = doc_ref.get()
        if doc.exists:
            print(f"‚ö° Cache Hit! Loading results for {target_domain}.")
            data = doc.to_dict()
            return data.get("competitors", [])

    print(f"üê¢ Cache Miss. Running full analysis for {target_domain}...")

    prompt = (
        f"I need to identify the top 5 direct competitors for {target_domain}. "
        f"Do NOT guess based on the name.\n\n"
        f"STEP 1: Use `scrape_website` to analyze {target_domain}. Identify their specific industry.\n"
        f"STEP 2: Use `Google Search` to find 5 actual competitors in that industry.\n"
        f"SAFETY NET: If Search fails, use internal training data.\n"
        f"STEP 3: Return JSON with keys: 'industry_profile' (string) and 'competitors' (list of objects with name, domain, reason)."
    )

    try:
        raw_response = await agent.run_agent_turn(prompt, [], headless=True)

        data = {}
        json_match = re.search(r"\{.*\}", raw_response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                print("‚ö†Ô∏è JSON Error in Discovery. Trying Python eval...")
                try:
                    data = ast.literal_eval(json_str)
                except Exception:
                    print("‚ùå Discovery parsing failed completely.")
                    return []

        competitors = data.get("competitors", [])
        profile = data.get("industry_profile", "Unknown")

        valid_competitors = []
        for comp in competitors:
            if "company" in comp and "name" not in comp:
                comp["name"] = comp["company"]

            if not comp.get("domain"):
                clean_name = (
                    comp.get("name", "").replace(" ", "").replace(",", "").lower()
                )
                comp["domain"] = f"{clean_name}.com"

            if comp.get("name") != "Unknown":
                valid_competitors.append(comp)

        if db and valid_competitors:
            db.collection(CACHE_COLLECTION).document(target_domain).set(
                {
                    "industry_profile": profile,
                    "competitors": valid_competitors,
                    "dismissed": [],
                    "last_updated": datetime.now(),
                }
            )
            print(f"‚úÖ Saved {len(valid_competitors)} competitors to cache.")
            return valid_competitors
        else:
            print("‚ö†Ô∏è Discovery finished but found 0 valid competitors.")
            return []

    except Exception as e:
        print(f"‚ùå Discovery failed: {e}")
        return []


# --- CORE LOGIC: REFRESH (Bulletproof Parsing & Domain Guessing) ---
async def refresh_competitors(target_domain, target_count=5, retry_level=0):
    print(f"üîÑ Refreshing list for {target_domain} (Attempt: {retry_level})...")
    if not db:
        return []

    doc_ref = db.collection(CACHE_COLLECTION).document(target_domain)
    doc = doc_ref.get()

    existing_competitors = []
    dismissed_domains = []
    industry_profile = "a specific industry"

    if doc.exists:
        data = doc.to_dict()
        existing_competitors = data.get("competitors", [])
        dismissed_domains = data.get("dismissed", [])
        industry_profile = data.get("industry_profile", industry_profile)

    existing_competitors = [
        c for c in existing_competitors if c.get("name") != "Unknown"
    ]

    needed = target_count - len(existing_competitors)
    if needed <= 0:
        return existing_competitors

    ask_for = max(needed, 3) + (retry_level * 2)

    current_names = [c["name"] for c in existing_competitors]
    current_domains_norm = [
        d.lower()
        .replace("https://", "")
        .replace("http://", "")
        .replace("www.", "")
        .split("/")[0]
        for d in [c["domain"] for c in existing_competitors]
    ]
    dismissed_norm = [
        d.lower()
        .replace("https://", "")
        .replace("http://", "")
        .replace("www.", "")
        .split("/")[0]
        for d in dismissed_domains
    ]

    banned_list = ", ".join(current_names)

    instruction_modifier = ""
    if retry_level > 0:
        instruction_modifier = "‚ö†Ô∏è PREVIOUS ATTEMPT FAILED. You used single quotes or missing keys. USE VALID JSON with double quotes. Find niche players."

    prompt = (
        f"I need {ask_for} NEW competitors for {target_domain} (Industry: {industry_profile}).\n"
        f"ALREADY LISTED (BANNED): {banned_list}.\n"
        f"{instruction_modifier}\n\n"
        f"INSTRUCTION: Find {ask_for} companies that are NOT in the BANNED list above.\n"
        f"You MUST return a JSON object with this EXACT schema:\n"
        f"{{ 'competitors': [ {{ 'name': 'Company Name', 'domain': 'company.com', 'reason': 'Why they match' }} ] }}\n"
        f"DO NOT use keys like 'company' or 'description'. YOU MUST FIND THE DOMAIN."
    )

    try:
        raw_response = await agent.run_agent_turn(prompt, [], headless=True)

        data = {}
        json_match = re.search(r"\{.*\}", raw_response, re.DOTALL)

        if json_match:
            json_str = json_match.group(0)
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                print("‚ö†Ô∏è JSON Error. Trying Python eval for single quotes...")
                try:
                    data = ast.literal_eval(json_str)
                except Exception:
                    print("‚ùå Parsing failed completely.")
                    data = {}

        candidates = data.get("competitors", [])
        valid_new_additions = []

        for cand in candidates:
            if "company" in cand and "name" not in cand:
                cand["name"] = cand["company"]

            if "domain" not in cand or not cand["domain"]:
                print(f"‚ö†Ô∏è Guessing domain for {cand.get('name')}")
                clean_name = (
                    cand.get("name", "").replace(" ", "").replace(",", "").lower()
                )
                cand["domain"] = f"{clean_name}.com"

            raw_domain = cand.get("domain", "").lower()

            if not raw_domain or raw_domain == "unknown":
                continue

            norm_domain = (
                raw_domain.replace("https://", "")
                .replace("http://", "")
                .replace("www.", "")
                .split("/")[0]
            )

            if norm_domain in current_domains_norm:
                print(f"‚ö†Ô∏è Skipping duplicate: {raw_domain}")
                continue
            if norm_domain in dismissed_norm:
                print(f"‚ö†Ô∏è Skipping dismissed: {raw_domain}")
                continue
            if target_domain in raw_domain:
                continue

            valid_new_additions.append(cand)

        final_additions = valid_new_additions[:needed]

        if final_additions:
            full_list = existing_competitors + final_additions
            doc_ref.update({"competitors": full_list, "last_updated": datetime.now()})
            print(f"‚úÖ Added {len(final_additions)} competitors.")
            return full_list
        else:
            print("‚ö†Ô∏è All candidates were filtered out.")
            if retry_level < 2:
                print(f"üîÑ Triggering retry level {retry_level + 1}...")
                return await refresh_competitors(
                    target_domain, target_count, retry_level=retry_level + 1
                )

            return existing_competitors

    except Exception as e:
        print(f"‚ùå Refresh failed: {e}")
        return existing_competitors


# --- CORE LOGIC: SURGICAL REMOVE ---
def remove_competitor_from_cache(target_domain, competitor_domain_to_remove):
    if not db:
        return

    try:
        doc_ref = db.collection(CACHE_COLLECTION).document(target_domain)
        doc = doc_ref.get()

        if doc.exists:
            data = doc.to_dict()
            current_list = data.get("competitors", [])
            dismissed_list = data.get("dismissed", [])

            new_list = [
                c
                for c in current_list
                if c.get("domain") != competitor_domain_to_remove
            ]

            if competitor_domain_to_remove not in dismissed_list:
                dismissed_list.append(competitor_domain_to_remove)

            doc_ref.update({"competitors": new_list, "dismissed": dismissed_list})
            print(f"üóëÔ∏è Blacklisted {competitor_domain_to_remove} for {target_domain}")
    except Exception as e:
        print(f"‚ùå Error updating cache: {e}")


# --- ANALYSIS HELPERS ---
async def check_linkedin_updates(company_name, domain):
    print(f"\n--- üïµÔ∏è‚Äç‚ôÇÔ∏è Starting LinkedIn Check for {company_name} ---")
    tracker = LinkedInTracker(agent_module=agent, use_mock=False)
    try:
        return await tracker.get_company_updates(company_name, domain)
    except Exception:
        return None


async def analyze_competitor_website(domain):
    urls_to_try = [f"https://{domain}", f"https://www.{domain}"]
    for url in urls_to_try:
        try:
            analysis_prompt = (
                f"Analyze the website {url} using the scrape_website tool. "
                f"If the scrape returns empty or 'blocked', use your internal knowledge. "
                f"Return a JSON object with keys: 'name', 'value_proposition', 'solutions', 'industries'. "
            )
            raw_response = await agent.run_agent_turn(
                analysis_prompt, [], headless=True
            )
            json_match = re.search(r"\{.*\}", raw_response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group(0))
                if data.get("value_proposition") not in ["N/A", "BLOCKED", None]:
                    return SimpleNamespace(**data)
        except Exception:
            pass

    try:
        fallback_prompt = f"Act as CMO. Profile '{domain}'. Return JSON keys: 'name', 'value_proposition', 'solutions', 'industries'."
        raw_response = await agent.run_agent_turn(fallback_prompt, [], headless=True)
        json_match = re.search(r"\{.*\}", raw_response, re.DOTALL)
        if json_match:
            return SimpleNamespace(**json.loads(json_match.group(0)))
    except Exception:
        pass
    return SimpleNamespace(
        name=domain,
        value_proposition="Analysis currently unavailable.",
        solutions="N/A",
        industries="N/A",
    )


# --- MAIN JOB: REPORT BY EXCEPTION ---
async def run_daily_brief():
    print(f"üöÄ Starting Daily Surveillance: {datetime.now()}")
    if not db:
        return

    comp_docs = db.collection("competitors").stream()
    competitors = [doc.id for doc in comp_docs]
    memory = utils.load_memory()

    if REFERENCE_DOMAIN in competitors:
        competitors.remove(REFERENCE_DOMAIN)

    updates_detected = 0

    for domain in competitors:
        print(f"--- Surveillance: {domain} ---")
        company_name = domain.split(".")[0].capitalize()

        # 1. Run Analysis
        website_result = await analyze_competitor_website(domain)
        linkedin_result = await check_linkedin_updates(company_name, domain)

        # 2. Compare with Memory
        prev_data = memory.get(domain, {})
        if isinstance(prev_data, str):
            prev_val_prop = prev_data
        else:
            prev_val_prop = prev_data.get("content", {}).get("value_proposition")

        current_val_prop = getattr(website_result, "value_proposition", "N/A")

        # Detect Changes
        website_changed = False
        if prev_val_prop and prev_val_prop != "N/A":
            if (current_val_prop != prev_val_prop) and (
                "Analysis currently unavailable" not in current_val_prop
            ):
                website_changed = True

        li_summary = (
            linkedin_result.summary_text
            if (linkedin_result and len(linkedin_result.summary_text) > 50)
            else ""
        )
        news_found = bool(li_summary and "No recent updates" not in li_summary)

        # 3. Update Memory
        full_company_data = {
            "name": getattr(website_result, "name", domain),
            "content": {
                "value_proposition": current_val_prop,
                "solutions": getattr(website_result, "solutions", "N/A"),
                "industries": getattr(website_result, "industries", "N/A"),
            },
            "linkedin_update": li_summary or "No recent updates.",
            "last_updated": datetime.now().isoformat(),
        }

        if "Analysis currently unavailable" not in current_val_prop:
            memory[domain] = full_company_data

        # 4. REPORT ON EXCEPTION
        if website_changed or news_found:
            print(f"üîî CHANGE DETECTED for {company_name}!")
            updates_detected += 1

            change_context = ""
            if website_changed:
                change_context += f"*Website Strategy Shift:*\nOld: {prev_val_prop}\nNew: {current_val_prop}\n\n"
            if news_found:
                change_context += f"*News/Social Update:*\n{li_summary}"

            summary_prompt = f"Act as a competitive intelligence analyst. Summarize this update for {company_name} in 2 bullet points. Focus on the strategic implication:\n{change_context}"
            analyst_note = await agent.run_agent_turn(
                summary_prompt, [], headless=False
            )

            post_update_to_slack(
                company_name, analyst_note, deep_dive_url=f"https://{domain}"
            )
            send_update_email(
                company_name, analyst_note, deep_dive_url=f"https://{domain}"
            )
        else:
            print(f"üí§ No significant changes for {company_name}.")

        await asyncio.sleep(5)

    utils.save_memory(memory)
    print(f"üíæ Surveillance Complete. {updates_detected} updates sent.")


if __name__ == "__main__":
    asyncio.run(run_daily_brief())
